### Evaluation Result

| Model Type | Model Name | Score |
|------------|------------|-------|
| llama3 based | meta-llama/Meta-Llama-3-8B-Instruct | 16/20 = 80 |
| llama3 based | UnicomLLM/Unichat-llama3-Chinese-8B | 11/20 = 55 |
| llama2 based | meta-llama/Llama-2-7b-chat-hf | 10/20 = 50 |
| llama2 based | meta-llama/Llama-2-13b-chat-hf | - |
| llama2 based | taide/TAIDE-LX-7B-Chat | - |
| llama2 based | yentinglin/Taiwan-LLM-7B-v2.1-chat | 5/20 = 25 |
| llama2 based | yentinglin/Taiwan-LLM-13B-v2.0-chat | - |
| Mistral based | mistralai/Mistral-7B-Instruct-v0.2 | 15/20 = 75 |
| Mistral based | MediaTek-Research/Breeze-7B-Instruct-v1_0 | - |
| Gemma based | google/gemma-1.1-7b-it | 16/20 = 80 |