### Evaluation Result

| Model Type | Model Name | Sample 20 Acc | Sample 50 Acc |
|------------|------------|-------| -------- |
| llama3 based | meta-llama/Meta-Llama-3-8B-Instruct | 16/20 = 80 | 41/50 = 82 (normal, bad) |
| llama3 based | UnicomLLM/Unichat-llama3-Chinese-8B | 11/20 = 55 | - |
| llama2 based | meta-llama/Llama-2-7b-chat-hf | 10/20 = 50 | - |
| llama2 based | meta-llama/Llama-2-13b-chat-hf | - | - |
| llama2 based | taide/TAIDE-LX-7B-Chat | - | - |
| llama2 based | yentinglin/Taiwan-LLM-7B-v2.1-chat | 5/20 = 25 | - |
| llama2 based | yentinglin/Taiwan-LLM-13B-v2.0-chat | - | - |
| Mistral based | mistralai/Mistral-7B-Instruct-v0.2 | 15/20 = 75 | - |
| Mistral based | MediaTek-Research/Breeze-7B-Instruct-v1_0 | 16/20 = 80(fastest) | 45/50 = 90 (fastest, prettiest) |
| Gemma based | google/gemma-1.1-7b-it | 16/20 = 80 | 44/50 = 88 (normal, prettier) |